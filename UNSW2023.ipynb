{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Parall Programming (NCI-ResTech 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# the jupyter notebook is launched from your $HOME, change the working directory provided a username directory is created under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/Parallel-Programming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenMP\n",
    "Our example ([monte-carlo-pi-serial](./monte-carlo-pi-serial.c)) for you to get a hang of  parallel programming is slightly more complicated than a helloword program. Nevertheless, it is a simple snippet showcasing a basic openmp program. \n",
    "\n",
    "The program approximates Pi by Monte-Carlo method. Run the next cell to compile and execute the serial code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make clean && make mc-serial && echo \"Compilation Successful!\" && ./monte-carlo-pi-serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multithreading version is implemented at ([monte-carlo-pi-openmp.c](./monte-carlo-pi-openmp.c)) by OpenMP. In essence, $N$ number of randowm numbers are distributed to multiple threads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to compile the OpenMP code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make clean && make mc-omp && echo \"Compilation Successful!\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the program with a fixed number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMP_NUM_THREADS=12 ./monte-carlo-pi-openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenACC\n",
    "Now we offload the computation to a GPU to accelerate the for-loop. To this end, firstly we need to load NVIDIA HPC STK module on Gadi.\n",
    "\n",
    "**`TODO`**: Refactor [monte-carlo-pi-openacc.c](./monte-carlo-pi-openacc.c) by changing to OpenACC clauses. \n",
    "\n",
    "Since we will compile with managed memory, there's no need to include data transfer clauses. But this will come to an issue for gaining more performance.\n",
    "\n",
    "The following flags are used in compiling the OpenACC code:\n",
    "\n",
    "-Minfo=accel: Show the information about the accelerated code by OpenACC\n",
    "\n",
    "-ta:telsa=mamaged: Target OpenACC to Nvidia GPUs with mamanged memory\n",
    "\n",
    "We also use NVTX libray which provides annotations for profiling the code.\n",
    "\n",
    "If you are getting stuck, peek the solution at ([solution](./solution/monte-carlo-pi-openacc.c))\n",
    "\n",
    "Once you have rendered the code with correct OpenACC, run the next cell to compile and execute the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make clean && make mc-acc && echo \"Compilation Successful!\" && ./monte-carlo-pi-openacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will demonstrate how to submit a batch job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MPI\n",
    "Our last parallel programming model uses MPI. The total $N$ number of random numbers are split into multiple processors. Each process independtely calculates the number of random numbers that are locally stored witthin the process. The results of each individual MPI rank are collected and summed at a root process (MPI rank 0). \n",
    "\n",
    "Look out for the following parts in the program ([monte-carlo-pi-mpi.c](./monte-carlo-pi-mpi.c)).\n",
    "```cpp\n",
    "#include <mpi.h>\n",
    "\n",
    "MPI_Init(&argc, &argv);\n",
    "\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "MPI_Wtime();\n",
    "\n",
    "MPI_Reduce(&count, &count_tot, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "MPI_Barrier(MPI_COMM_WORLD);\n",
    "\n",
    "MPI_Finalize();\n",
    "```\n",
    "\n",
    "Run the next cell to excute the MC_pi program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make clean && make mc-mpi && echo \"Compilation Successful!\" && mpiexec -np 4 ./monte-carlo-pi-mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile with mpiP\n",
    "\n",
    "Run the next cell to inspect the profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *.mpiP\n",
    "!rm -r *.mpiP"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
