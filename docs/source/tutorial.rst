Tutorial
========

In this tutorial, we'll be using the Gadi HPC machine at NCI. A Python virtual environment will be provided for you during the session.


High-level HPC Architecture
---------------------------

In an HPC machine, nodes are provisioned by allocating compute resources from a central pool based on the job's requirements. The system uses 
job schedulers like Slurm or PBS to manage and distribute these resources efficiently. When a job is submitted, the scheduler assigns nodes with 
the necessary compute power, memory, and storage to execute the job. Nodes can be dynamically allocated or deallocated based on job demands and system load.

.. image::  figs/HPC_overview.drawio.png


**Login nodes** in an HPC cluster serve as access points for users to interact with the system, allowing tasks such as code compilation, data preparation, 
and job submission. These nodes are configured with resource limits to ensure they remain responsive and are not used for heavy computations. Users connect 
to login nodes via SSH to submit and manage jobs through the scheduler, which then allocates resources from compute nodes for actual processing. While login 
nodes handle light, interactive tasks, the primary computational work occurs on separate, dedicated nodes. These nodes also play a role in maintaining system 
security and stability through regular updates and restricted access.

On the other hand, **Compute nodes** are dedicated to executing intensive computational tasks. They are equipped with powerful processors, ample RAM, and high-speed 
networking to handle large-scale data processing and parallel computations. Managed by a job scheduler, compute nodes are allocated based on job requirements 
and are dedicated to a specific job. 

**Storage nodes** in an HPC cluster are essential for managing and providing access to large volumes of data generated by computational tasks. They use various storage 
technologies, including local disks for temporary storage and networked or parallel file systems like Lustre or GPFS for scalable, high-performance data access. 
These nodes are optimized for high throughput and low latency, integrated with the cluster's networking to facilitate fast data transfer. 


Requesting a Job
****************

1.  Which project are you using?
2.  Which job queue are you planning to use?
3.  How many CPU cores are required for your task?
4.  How many GPUs do you need?
5.  What is the estimated runtime of your program?
6.  Which modules are necessary to execute the program?
7.  What script or command will you use to run the program?


.. code-block:: console
    :linenos:

    pygments_style = 'sphinx'

    #!/bin/bash

    #PBS -P vp91
    #PBS -q normal
    #PBS -l ncpus=48
    #PBS -l mem=10GB
    #PBS -l walltime=00:02:00
    #PBS -N testScript

    module load python3/3.11.0
    module load papi/7.0.1

    . /scratch/vp91/Training-Venv/intro-parallel-prog/bin/activate

    which python

* **P**     - Gadi project (sometimes called account) used
* **q**     - Gadi queue to use
* **ncpus** - Total number of cores requested
* **ngpus** - Total number of GPUs requested
* **mem**   - Total memory requested
* **l**     - Total wall time for which the resources are provisioned
* **N**     - Name of the job 

For more PBS Directives please checkout the `Gadi document <https://opus.nci.org.au/display/Help/PBS+Directives+Explained>`_  and for more details on the 
different Gadi queues plase checkout the corresponding `Gadi document <https://opus.nci.org.au/display/Help/Queue+Structure>`_ .

All the python code are available in the directory python/src while all the job scripts are available in the directory. To submit a job use 
the command

.. code-block:: console
    :linenos:

    qsub 0_testScript.pbs

and to know the status of your job use the command

.. code-block:: console
    :linenos:

    qstat <jobid>

To know get the details of the nodes allocated use the command

.. code-block:: console
    :linenos:

    qstat -swx <jobid>

HPC Compute Node
****************

.. image::  figs/computeNode.drawio.png

A cache is a small, high-speed storage component used to temporarily hold frequently accessed data or instructions
(`temporal locality <https://www.sciencedirect.com/topics/computer-science/temporal-locality>`_ ) to improve system performance. Its primary purpose is to 
reduce the time it takes for a processor to access data from the main memory (RAM) or other slower storage devices.



L1, L2, and L3 caches are hierarchical levels of CPU cache designed to speed up data access and improve overall processing performance:

- **L1 Cache**: This is the smallest and fastest cache level, located directly on the CPU chip. It typically includes separate caches for instructions (L1i) and data (L1d). Due to its proximity to the CPU cores, it provides the quickest access to frequently used data and instructions, but it has limited capacity.

- **L2 Cache**: Larger than L1 but slower, the L2 cache is also located on the CPU chip or very close to it. It serves as an intermediary between the fast L1 cache and the slower L3 cache or main memory. It holds data and instructions that are not immediately needed by L1 but are accessed frequently enough to justify faster access than the main memory.

- **L3 Cache**: This is the largest and slowest of the three caches, typically shared among multiple CPU cores. It acts as a last-level cache before data is fetched from main memory. The L3 cache improves performance by storing a larger amount of data that is likely to be used by multiple cores, thus reducing the number of memory accesses and potential bottlenecks.

Together, these cache levels balance speed and capacity to enhance CPU performance by minimizing data access times.

How does cache influence peformance?
************************************

In the context of caching, **cache hit** and **cache miss** refer to the outcomes of a cache lookup operation:

- **Cache Hit**: A cache hit occurs when the data or instruction requested by the CPU is found in the cache. This means the cache contains a copy of the data that is needed, allowing the CPU to access it quickly and avoid fetching it from the slower main memory. Cache hits improve performance by reducing access time and latency.

- **Cache Miss**: A cache miss happens when the requested data or instruction is not found in the cache. In this case, the system must retrieve the data from the main memory or another slower storage medium. After fetching the data, it is typically stored in the cache for future use. Cache misses can result in slower access times since the data must be retrieved from a less efficient source.

Overall, maximizing cache hits and minimizing cache misses are key strategies for optimizing system performance and efficiency. Also, as the data size increases, 
cache misses also increase, leading to performance degradation.

.. code-block:: console
    :linenos:
    
    qsub 1_cachePapi.pbs

Are you getting linear peformance for third and fourth call?

Vector Parallelism
------------------

Vector parallelism is a form of parallel computing that leverages the simultaneous processing of multiple data elements using vector processors or 
SIMD (Single Instruction, Multiple Data) instructions. 

.. image::  figs/vectorPrallelism.drawio.png


How does vectorization influence peformance?
*******************************************

Vector processors are specialized hardware designed to handle vector operations. Instead of processing single data elements sequentially, they can operate 
on entire vectors (arrays of data) in parallel. The effectiveness of vector parallelism depends on the vector length, which is the number of data elements 
a vector processor can handle in parallel.

We will use `Numba` to vectorrize python code.

.. code-block:: console
    :linenos:
    qsub 2_vectorize.pbs


Multi-core Parallelism
----------------------

.. image::  figs/multicorePrallelism.drawio.png


GPU Parallelism 
---------------
Gadi only has NVIDIA GPUs. So when we say GPUs we mean NVIDIA GPUs. Neveretheless, many concepts discussed here are the same across different vendors_.
While CPU is optimized to do a single operation as fast as it can (low latency operation), GPU is optimized to do large number of slow operations (high throughput operation).
GPUs  are composed of multiple Streaming Multiprocessors (SMs), an on-chip L2 cache, and high-bandwidth DRAM. The SMs execute operations and the data and code are accessed from DRAM through the L2 cache.

.. image::  figs/SM.png

Each SM is organized into CUDA cores capable of doing specialized operations.

.. image::  figs/cuda_cores.png

GPU Execution Model
*******************

Each GPU kernels are launched with a set of threads. The threads can be organized into blocks, and the blocks can be organized into a grid. The maximum number of threads a block can have will depend on the GPU generation. 

.. image::  figs/blocks.png

A block can be executed only in one SM, but an SM can have multiple blocks simultaneously. The maximum number of blocks an SM can host will depend on the GPU generation. Since an SM can execute multiple thread blocks concurrently, it is always a good idea to launch a kernel with blocks several times higher than the number of SMs. 

.. image:: figs/wave.png

**Wave** is the number of thread blocks that run concurrently. So if we have 12 SMs and we launch a kernel with 8 blocks, with an occupency of 1 block per SM, there will be two waves.


Thread Indexing
***************

Threads, blocks, and grids are organized in three dimensions: x, y, and z. For simplicity, we will use only two dimensions.

**Dimensions**:

1.  *gridDim.x* — blocks in the x dimension of the grid 
2.  *gridDim.y* — blocks in the y dimension of the grid 
3.  *blockDim.x* — threads in the x dimension of the block 
4.  *blockDim.y* — threads in the y dimension of the block 

**Indexing**: 

1.  *blockIdx.x* — block index in x dimension 
2.  *blockIdx.y* — block index in y dimension 
3.  *threadIdx.x* — thread index in x dimension 
4.  *threadIdx.y* — thread index in y dimension 

How do we assign a unique thread id to each thread using the above?
-------------------------------------------------------------------

.. image::  figs/thread_index.drawio.png


1. Find the blockId --> 
.. code-block:: console
    blockId  = (gridDim.y * blockIdx.x) + blockIdx.y

2. Using the blockId, find the threadId 
.. code-block:: console
    threadId = [(blockDim.x * blockDim.y) * blockId] + [(blockDim.y * threadIdx.x) + threadIdx.y]

Warps and Warp Schedulers
*************************

While we can arrange the threads in any order, the SM schedules the threads as **Warps**, and each warp contains 32 threads. For example, if you launch a block with 256 threads, those 256 threads are arranged as 8 warps (256/8). All the threads in the same warp can only execute the same instruction at a given time. For example, if we have a program

.. code-block:: console
    a = b + c
    d = x * y

*All* the threads in the warp should finish executing the addition operation, only then can the threads execute the multiplication operation. Depending on the generation of the GPU, it may contain more than one warp scheduler. For instance, in the *Fermi GPU*, each SM features two warp schedulers and two instruction dispatch units. This allows two warps to be issued and executed concurrently. It is always a good idea to consider the warp size (32) and the maximum number of concurrent warps possible when deciding the block size.

.. image::  figs/warp.png

Data Movement in GPUs
*********************

.. image::  figs/gpu-node.png

The are two types of data movement in GPUs:

1.  Host-to-Device data movement (H2D): Move data from the host memory to the GPU memory.
2.  Device-to-Device data movement (D2D): Move data from the memory of one GPU to another.

H2D transfer happens through the PCIe switch and D2D transfer happens through NVLink. This makes D2D transfers more faster than H2D transfers.

Streams
*******

.. image::  figs/streams.png


Multi-node Parallelism
-----------------------

While all the aforementioned parallelism is beneficial, it is limited to a single node. To truly scale up an application, we need to use multiple nodes, i.e., distributed computing. The main challenge with distributed computing is that the memory in each node is distinct and separate, meaning there is no way for a thread in one node to access data in another node.

.. image::  figs/multinodePrallelism.drawio.png

We overcome this challenge by using message passing.

.. image::  figs/MPI.png

Broadcast Operation
*******************

.. image::  figs/bcast.png

### GPU-aware MPI and All-Gather Operation

.. image:: figs/allgather.png

# Reference
1. https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html
2. https://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf
3. https://www.sciencedirect.com/science/article/abs/pii/B978012800979600010X
4. https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf
5. https://mpitutorial.com


# Contributers
1. [Joseph John, Staff Scientist, NCI](https://www.josephjohn.org) \

*ChatGPT has been utilized to enhance and generate texts in this document*.





